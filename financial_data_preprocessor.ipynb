{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16fc60c0",
   "metadata": {},
   "source": [
    "#### Financial Market Data Pre-Processor\n",
    "\n",
    "##### Software Pre-requisites:\n",
    "```\n",
    "pip install pandas\n",
    "pip install finnhub-python\n",
    "```\n",
    "\n",
    "##### Running instructions \n",
    "- Press `Run All` Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ab90cdc-f2b3-41ba-a4a4-0eff99765e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import finnhub\n",
    "import pandas as pd\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# dates for request parameters\n",
    "start_time = time.time()\n",
    "date_today = date.today()\n",
    "today_date = date_today.strftime('%Y-%m-%d')\n",
    "current_year = str(date_today.year)\n",
    "\n",
    "from_time_unix = int(time.mktime((date_today - timedelta(weeks = 52)).timetuple()))\n",
    "to_time_unix = int(time.mktime(date_today.timetuple()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ded1bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnhub clients setup - https://finnhub.io/docs/api/\n",
    "def generate_finnhub_clients():\n",
    "    finnhub_keys = ['Y2Q0b3FpYWFkM2k5OGpodTJwZ2djZDRvcWlhYWQzaTk4amh1MnBoMA==','Y2UzbWQyYWFkM2kxaDJuN24xODBjZTNtZDJhYWQzaTFoMm43bjE4Zw==','Y2R0ZG1yMmFkM2k0MXY3aG9nM2djZHRkbXIyYWQzaTQxdjdob2c0MA==',\n",
    "        'Y2U1NmViaWFkM2lmZHZ0aHQzcjBjZTU2ZWJpYWQzaWZkdnRodDNyZw==','Y2U0dTFiMmFkM2llMTg4dGY4bmdjZTR1MWIyYWQzaWUxODh0ZjhvMA==','Y2UzbWU2MmFkM2kxaDJuN24xc2djZTNtZTYyYWQzaTFoMm43bjF0MA==',\n",
    "        'Y2UzbWtiYWFkM2kxaDJuN240ZGdjZTNta2JhYWQzaTFoMm43bjRlMA==','Y2U1NjlxMmFkM2lmZHZ0aHQwZTBjZTU2OXEyYWQzaWZkdnRodDBlZw==','Y2UzbXYxYWFkM2kxaDJuN244dWdjZTNtdjFhYWQzaTFoMm43bjh2MA==',\n",
    "        'Y2U0dHE5YWFkM2llMTg4dGY0YzBjZTR0cTlhYWQzaWUxODh0ZjRjZw==','Y2U0dTQyaWFkM2llMTg4dGZhYWdjZTR1NDJpYWQzaWUxODh0ZmFiMA==','Y2U1NmQ2cWFkM2lmZHZ0aHQzM2djZTU2ZDZxYWQzaWZkdnRodDM0MA==']\n",
    "\n",
    "    finnhub_client_list = []\n",
    "\n",
    "    for key in finnhub_keys:\n",
    "        finnhub_client_list.append(finnhub.Client(api_key=base64.b64decode(key).decode()))\n",
    "\n",
    "    return finnhub_client_list\n",
    "\n",
    "finnhub_clients = generate_finnhub_clients()\n",
    "client_num = 0\n",
    "\n",
    "def get_finhub_client():\n",
    "    \"\"\" returns a finnhub client to perform requests to gather financial data \"\"\"\n",
    "\n",
    "    global client_num\n",
    "    if client_num >= len(finnhub_clients) - 1:\n",
    "        client_num = 0\n",
    "    else:\n",
    "        client_num+=1\n",
    "\n",
    "    return finnhub_clients[client_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "061aaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_path(csv_path):\n",
    "    csv_file_path = Path(csv_path)\n",
    "    csv_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "376e81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_candlestick_data_frame(ticker_symbol):\n",
    "    \n",
    "    candle_response = get_finhub_client().stock_candles(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix)\n",
    "\n",
    "    candles = pd.json_normalize(candle_response)[['t', 'o', 'c', 'h', 'l', 'v']]\n",
    "    \n",
    "    candles_df = pd.DataFrame(columns=['date','unix_time', 'open', 'close', 'high', 'low', 'volume'])\n",
    "    \n",
    "    candles_df['unix_time'] = candles['t'][0]\n",
    "    candles_df['open'] = candles['o'][0]\n",
    "    candles_df['close'] = candles['c'][0]\n",
    "    candles_df['high'] = candles['h'][0]\n",
    "    candles_df['low'] = candles['l'][0]\n",
    "    candles_df['volume'] = candles['v'][0]\n",
    "\n",
    "    candles_df.drop_duplicates(subset=['unix_time'], keep='last', inplace=True)\n",
    "    candles_df.sort_values(by=['unix_time'], ascending=False, inplace=True)    \n",
    "\n",
    "    candles_df['date'] = pd.to_datetime(candles_df['unix_time'],unit='s').astype(str)    \n",
    "\n",
    "    return candles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aecd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_candlestick_data(ticker_symbol):\n",
    "    \"\"\" populates the daily candlestick data for the given stock into the file: 'data/candlestick_data.csv' \"\"\"\n",
    "\n",
    "    candlestick_df = retrieve_candlestick_data_frame(ticker_symbol)\n",
    "\n",
    "    if not(candlestick_df.empty):\n",
    "\n",
    "        candlestick_df.insert(0,'symbol', ticker_symbol)\n",
    "\n",
    "        csv_file_path = create_csv_path(\"data/candlestick_data.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, candlestick_df], axis=0).drop_duplicates(subset=['symbol', 'unix_time'], keep='last').sort_values(by=['symbol', 'unix_time'], ascending=[True, False])\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                candlestick_df.sort_values(by=['symbol', 'unix_time'], ascending=[True, False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating candlestick data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79708c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_technical_data_frame(ticker_symbol):\n",
    "    \n",
    "    bband_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='bbands', indicator_fields={\"timeperiod\": 20, \"seriestype\":\"c\"})\n",
    "    bband_df = pd.json_normalize(bband_response)[['t', 'o', 'c', 'h', 'l', 'v', 'lowerband', 'middleband', 'upperband']]\n",
    "\n",
    "    ema_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='ema', indicator_fields={\"timeperiod\": 20, \"seriestype\":\"c\"})\n",
    "    ema_df = pd.json_normalize(ema_response)[['ema']]\n",
    "\n",
    "    rsi_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='rsi', indicator_fields={\"timeperiod\": 18, \"seriestype\":\"c\"})\n",
    "    rsi_df = pd.json_normalize(rsi_response)[['rsi']]\n",
    "\n",
    "    adx_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='adx', indicator_fields={\"timeperiod\": 10})\n",
    "    adx_df = pd.json_normalize(adx_response)[['adx']]\n",
    "\n",
    "    atr_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='atr', indicator_fields={\"timeperiod\": 18})\n",
    "    atr_df = pd.json_normalize(atr_response)[['atr']]\n",
    "\n",
    "    cci_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='cci', indicator_fields={\"timeperiod\": 18})\n",
    "    cci_df = pd.json_normalize(cci_response)[['cci']]\n",
    "    \n",
    "    stoch_response = get_finhub_client().technical_indicator(symbol=ticker_symbol, resolution='D', _from=from_time_unix, to=to_time_unix, indicator='stoch')\n",
    "    stoch_df = pd.json_normalize(stoch_response)[['slowd', 'slowk']]\n",
    "    \n",
    "    technical_df = pd.DataFrame(columns=['date','unix_time', 'open', 'close', 'high', 'low', 'volume', 'lowerband', 'middleband', 'upperband', 'ema', 'rsi', 'adx', 'atr', 'cci', 'slowd', 'slowk'])\n",
    "    \n",
    "    technical_df['unix_time'] = bband_df['t'][0]\n",
    "    technical_df['open'] = bband_df['o'][0]\n",
    "    technical_df['close'] = bband_df['c'][0]\n",
    "    technical_df['high'] = bband_df['h'][0]\n",
    "    technical_df['low'] = bband_df['l'][0]\n",
    "    technical_df['volume'] = bband_df['v'][0]\n",
    "    technical_df['lowerband'] = bband_df['lowerband'][0]\n",
    "    technical_df['middleband'] = bband_df['middleband'][0]\n",
    "    technical_df['upperband'] = bband_df['upperband'][0]\n",
    "    technical_df['ema'] = ema_df['ema'][0]\n",
    "    technical_df['rsi'] = rsi_df['rsi'][0]\n",
    "    technical_df['adx'] = adx_df['adx'][0]\n",
    "    technical_df['atr'] = atr_df['atr'][0]\n",
    "    technical_df['cci'] = cci_df['cci'][0]\n",
    "    technical_df['slowd'] = stoch_df['slowd'][0]\n",
    "    technical_df['slowk'] = stoch_df['slowk'][0]\n",
    "\n",
    "    technical_df.drop_duplicates(subset=['unix_time'], keep='last', inplace=True)\n",
    "    technical_df.sort_values(by=['unix_time'], ascending=False, inplace=True)\n",
    "\n",
    "    technical_df = technical_df[technical_df['upperband'] > 0]\n",
    "\n",
    "    technical_df['date'] = pd.to_datetime(technical_df['unix_time'],unit='s').astype(str)\n",
    "\n",
    "    return technical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f206d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_technical_data(ticker_symbol):\n",
    "    \"\"\" populates the daily technical indicator data for the given stock into the file: 'data/technical_indicators.csv' \"\"\"\n",
    "\n",
    "    stock_technical_df = retrieve_technical_data_frame(ticker_symbol)\n",
    "\n",
    "    if not(stock_technical_df.empty):\n",
    "\n",
    "        stock_technical_df.insert(0,'symbol', ticker_symbol)\n",
    "\n",
    "        csv_file_path = create_csv_path(\"data/technical_indicators.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, stock_technical_df], axis=0).drop_duplicates(subset=['symbol', 'unix_time'], keep='last').sort_values(by=['symbol', 'unix_time'], ascending=[True, False])\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                stock_technical_df.sort_values(by=['symbol', 'unix_time'], ascending=[True, False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating technical indicator data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38cdab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_social_columns = {\"mention\": \"mention_reddit\", \"positiveScore\": \"positiveScore_reddit\",\"negativeScore\": \"negativeScore_reddit\",\n",
    "    \"positiveMention\": \"positiveMention_reddit\",\"negativeMention\": \"negativeMention_reddit\",\"score\": \"score_reddit\"}\n",
    "\n",
    "def retrieve_social_sentiment_data_frame(ticker_symbol):\n",
    "\n",
    "    social_response = get_finhub_client().stock_social_sentiment(ticker_symbol)\n",
    "\n",
    "    twitter_social_df = pd.json_normalize(social_response, record_path='twitter')\n",
    "    reddit_social_df = pd.json_normalize(social_response, record_path='reddit')\n",
    "\n",
    "    try:\n",
    "        if 'atTime' in twitter_social_df:\n",
    "            social_df = reddit_social_df.merge(twitter_social_df, how='left', on=['atTime'], suffixes=('', '_twitter')).fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "        else:\n",
    "            social_df = reddit_social_df.fillna(0).rename(columns=renamed_social_columns)\n",
    "\n",
    "    except Exception as error:\n",
    "        print(f\"Error while gathering social sentiment for {ticker_symbol} error: {error}\")\n",
    "        print(social_response)\n",
    "        \n",
    "        if 'atTime' in reddit_social_df:\n",
    "            social_df = reddit_social_df.fillna(0).rename(columns=renamed_social_columns)\n",
    "        else:\n",
    "            social_df = pd.DataFrame()\n",
    "\n",
    "    return social_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1242f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_social_sentiment(ticker_symbol):\n",
    "    \"\"\" populates the social sentiment for stocks on Reddit and Twitter for the given stock into the file: 'data/social_media_sentiment.csv' \"\"\"\n",
    "\n",
    "    social_df = retrieve_social_sentiment_data_frame(ticker_symbol)\n",
    "    if not(social_df.empty):\n",
    "        \n",
    "        social_df.insert(0,'symbol', ticker_symbol)\n",
    "        csv_file_path = create_csv_path(\"data/social_media_sentiment.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, social_df], axis=0).drop_duplicates(keep='last').sort_values(by=['symbol', 'atTime'], ascending=[True, False]).fillna(0)\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                social_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating social sentiment data for {ticker_symbol}, error: {error}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af1ddb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_trans_cols =['symbol','share','change','transactionDate','transactionCode','transactionPrice','name','filingDate','id']\n",
    "\n",
    "def populate_insider_transactions(ticker_symbol):\n",
    "    \"\"\" populates insider transactions into file 'data/insider_transactions.csv' \"\"\"\n",
    "\n",
    "    insider_transactions_response = get_finhub_client().stock_insider_transactions(ticker_symbol)\n",
    "\n",
    "    insider_trans_df = pd.json_normalize(insider_transactions_response, record_path='data')\n",
    "\n",
    "    if not insider_trans_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/insider_transactions.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, insider_trans_df], axis=0).drop_duplicates().sort_values(by=['symbol','transactionDate'], ascending=[True, False])[insider_trans_cols]\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                insider_trans_df.sort_values(by=['symbol', 'transactionDate'], ascending=[True, False])[insider_trans_cols].to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating insider transactions data for {ticker_symbol}, error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8afbb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_cols =['symbol','period','actual','estimate','surprise','surprisePercent']\n",
    "\n",
    "def populate_company_surprise_earnings(ticker_symbol):\n",
    "    \"\"\" populates company surprise earnings into file 'data/surprise_earnings.csv' \"\"\"\n",
    "\n",
    "    earnings_response = get_finhub_client().company_earnings(ticker_symbol)\n",
    "\n",
    "    earnings_df = pd.json_normalize(earnings_response)\n",
    "\n",
    "    if not earnings_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/surprise_earnings.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, earnings_df], axis=0).drop_duplicates(subset=['symbol', 'period'], keep='last').sort_values(by=['symbol','period'], ascending=[True, False])[earnings_cols]\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                earnings_df.sort_values(by=['symbol', 'period'], ascending=[True, False])[earnings_cols].to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating company earnings data for {ticker_symbol}, error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "824a696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_insider_sentiment(ticker_symbol):\n",
    "    \"\"\" populates the insider sentiment data into the file: 'data/insider_sentiment.csv' \"\"\"\n",
    "\n",
    "    insider_response = get_finhub_client().stock_insider_sentiment(ticker_symbol, \"2013-01-01\", current_year+\"-12-31\")\n",
    "\n",
    "    insider_df = pd.json_normalize(insider_response, record_path='data')\n",
    "\n",
    "    if not insider_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/insider_sentiment.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, insider_df], axis=0).drop_duplicates(keep='last').sort_values(by=['symbol','year','month'], ascending=[True,False,False])\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                insider_df.sort_values(by=['symbol','year','month'], ascending=[True,False,False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating insider sentiment data for {ticker_symbol}, error: {error}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce6b1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_columns = ['symbol', 'period', 'strongBuy', 'buy', 'hold', 'sell', 'strongSell']\n",
    "\n",
    "def populate_recommended_trends(ticker_symbol):\n",
    "    \"\"\" populates the latest analyst recommendation trends for a company into the file: 'data/recommendation_trends.csv' \"\"\"\n",
    "\n",
    "    trends = get_finhub_client().recommendation_trends(symbol=ticker_symbol)\n",
    "\n",
    "    trends_df = pd.json_normalize(trends)\n",
    "\n",
    "    if 'symbol' in trends_df and len(trends_df['symbol']) > 0:\n",
    "\n",
    "        trends_df[trends_columns]\n",
    "        csv_file_path = create_csv_path(\"data/recommendation_trends.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, trends_df], axis=0).sort_values(by=['symbol', 'period'], ascending=[True, False]).drop_duplicates(subset=['symbol', 'period'])[trends_columns]\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                trends_df.sort_values(by=['symbol', 'period'], ascending=[True, False]).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while updating recommendation trends data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccd02ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_basic_financials(ticker_symbol):\n",
    "    \"\"\" populates the basic financials of a company into the file: 'data/basic_company_financials.csv' \"\"\"\n",
    "\n",
    "    basic_financials_response = get_finhub_client().company_basic_financials(ticker_symbol, 'all')\n",
    "\n",
    "    if 'metric' in basic_financials_response and 'symbol' in basic_financials_response:\n",
    "\n",
    "        basic_fin_df = pd.DataFrame([basic_financials_response['metric']])\n",
    "        basic_fin_df.insert(0,'symbol', basic_financials_response['symbol'])\n",
    "        basic_fin_df['update_date'] = today_date\n",
    "\n",
    "        csv_file_path = create_csv_path(\"data/basic_company_financials.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, basic_fin_df], axis=0).sort_values(by=['symbol', 'update_date'], ascending=[True, False]).drop_duplicates(subset=['symbol'])\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                basic_fin_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating basic finanial data for {ticker_symbol}, error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42125235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_senate_lobbying(ticker_symbol):\n",
    "    \"\"\" populates the reported lobbying activities in the Senate and the House into the file: 'data/senate_lobbying.csv' \"\"\"\n",
    "\n",
    "    lobby_response = get_finhub_client().stock_lobbying(ticker_symbol, \"2000-01-01\", today_date)\n",
    "\n",
    "    lobby_df = pd.json_normalize(lobby_response, record_path='data')\n",
    "\n",
    "    if not lobby_df.empty:\n",
    "        csv_file_path = create_csv_path(\"data/senate_lobbying.csv\")\n",
    "\n",
    "        symbol_col = lobby_df.pop('symbol')\n",
    "        year_col = lobby_df.pop('year')\n",
    "        lobby_df.insert(0, 'symbol', symbol_col)\n",
    "        lobby_df.insert(0, 'year', year_col)\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(csv_file_path):\n",
    "                existing_df = pd.read_csv(csv_file_path, header = 0)\n",
    "                updated_df = pd.concat([existing_df, lobby_df], axis=0).sort_values(by=['symbol', 'year'], ascending=[True, False]).drop_duplicates().dropna(how='all', axis=1)\n",
    "                updated_df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "            else:\n",
    "                lobby_df.dropna(how='all', axis=1).to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error while generating senate lobbying data for {ticker_symbol}, error: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31c3f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_generate_data_for_ticker(stock_info, stock_tickers):\n",
    "    return 'symbol' in stock_info and len(stock_info['symbol']) > 0 and stock_info['symbol'] in stock_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c295718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Wait... Generating Financial Data. Interrupt the Program to Exit\n",
      "5 Symbols Processed. Current Run Time: 14.082903146743774 seconds\n",
      "10 Symbols Processed. Current Run Time: 33.98068714141846 seconds\n",
      "15 Symbols Processed. Current Run Time: 47.97073721885681 seconds\n",
      "20 Symbols Processed. Current Run Time: 62.01191592216492 seconds\n",
      "25 Symbols Processed. Current Run Time: 78.18243598937988 seconds\n",
      "30 Symbols Processed. Current Run Time: 98.97880697250366 seconds\n",
      "35 Symbols Processed. Current Run Time: 114.09027004241943 seconds\n",
      "40 Symbols Processed. Current Run Time: 130.11456418037415 seconds\n",
      "45 Symbols Processed. Current Run Time: 147.45365810394287 seconds\n",
      "50 Symbols Processed. Current Run Time: 163.39720702171326 seconds\n",
      "55 Symbols Processed. Current Run Time: 177.6875901222229 seconds\n",
      "60 Symbols Processed. Current Run Time: 191.17503309249878 seconds\n",
      "65 Symbols Processed. Current Run Time: 210.169193983078 seconds\n",
      "70 Symbols Processed. Current Run Time: 224.2913100719452 seconds\n",
      "75 Symbols Processed. Current Run Time: 238.5577368736267 seconds\n",
      "-- Exiting Program -- Total Execution Time: 4.059330467383067 minutes\n",
      "75 Stock Symbols Processed. Symbol List: ['MGM', 'ANGI', 'ATUS', 'PARA', 'V', 'YELP', 'F', 'SAVE', 'EA', 'MTN', 'JNJ', 'BYD', 'SOFI', 'SIRI', 'GME', 'BAC', 'SPOT', 'MRK', 'MTCH', 'TVTV', 'WFC', 'NFLX', 'DIS', 'T', 'SNAP', 'CVX', 'DISH', 'PG', 'CZR', 'INTC', 'AMD', 'EFX', 'DG', 'DKNG', 'UNH', 'LVS', 'AAPL', 'CHTR', 'NKE', 'LLY', 'CHDN', 'WOW', 'MSFT', 'CABO', 'NVDA', 'CRM', 'META', 'PFE', 'PENN', 'HD', 'VZ', 'AMZN', 'UBER', 'MSGS', 'LUMN', 'RCI', 'BMBL', 'GOOG', 'FUBO', 'NWSA', 'WBD', 'WWE', 'KO', 'MA', 'ROKU', 'PEP', 'LYFT', 'MANU', 'WYNN', 'CMCSA', 'XOM', 'TLSA', 'PINS', 'LBRDA', 'AMC']\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded set of symbols to generate data. Includes Comcast competitors and companies from the sectors listed below:\n",
    "# Ride Share, Gambling, Sports Entertainmnet, Big Tech, Social Media\n",
    "stock_tickers = {'AAPL','AMZN','AMC','AMD','ANGI','ATUS','BAC','BETZ','BMBL','BYD','CABO','CHDN','CHTR','CMCSA','CRM','CVX','CZR','DIS','DISH','DKNG','DG','EA','EFX',\n",
    "'F','FUBO','FWONA','GOOG','GME','HD','INTC','JNJ','JPN','KO','LBRDA','LUMN','LLY','LSXMA','LVS','LYFT','MANU','META','MA','MGM','MRK','MSFT','MSGS','MTCH','MTN',\n",
    "'NFLX','NKE','NVDA','NWSA','PARA','PENN','PEP','PDYPY','PFE','PG','PINS','RCI','ROKU','SAVE','SIRI','SNAP','SOFI','SPOT','T','TLSA','TVTV','TWTR','UBER','UNH',\n",
    "'V','VZ','WBD','WFC','WOW','WWE','WYNN','XOM','YELP'}\n",
    "\n",
    "all_stock_info = get_finhub_client().stock_symbols(exchange=\"US\", currency=\"USD\", security_type=\"Common Stock\")\n",
    "processed_symbols = []\n",
    "\n",
    "print(\"Please Wait... Generating Financial Data. Interrupt the Program to Exit\")\n",
    "\n",
    "for stock_info in all_stock_info:\n",
    "\n",
    "    if should_generate_data_for_ticker(stock_info, stock_tickers):\n",
    "        ticker_symbol = stock_info['symbol']\n",
    "\n",
    "        try:\n",
    "\n",
    "            populate_candlestick_data(ticker_symbol)\n",
    "\n",
    "            populate_technical_data(ticker_symbol)\n",
    "\n",
    "            populate_social_sentiment(ticker_symbol)\n",
    "\n",
    "            populate_insider_transactions(ticker_symbol)\n",
    "\n",
    "            populate_insider_sentiment(ticker_symbol)\n",
    "\n",
    "            populate_recommended_trends(ticker_symbol)\n",
    "\n",
    "            populate_company_surprise_earnings(ticker_symbol)\n",
    "\n",
    "            populate_basic_financials(ticker_symbol)\n",
    "\n",
    "            populate_senate_lobbying(ticker_symbol)\n",
    "\n",
    "            processed_symbols.append(ticker_symbol)\n",
    "\n",
    "            if len(processed_symbols) % 5 == 0:\n",
    "                print(f\"{len(processed_symbols)} Symbols Processed. Current Run Time: {(time.time() - start_time)} seconds\")\n",
    "                time.sleep(5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while generating data for {ticker_symbol}, error: {e}\")\n",
    "            time.sleep(10)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Process Interrupted\")\n",
    "            break\n",
    "\n",
    "print(f\"-- Exiting Program -- Total Execution Time: {(time.time() - start_time) / 60} minutes\")\n",
    "print(f\"{len(processed_symbols)} Stock Symbols Processed. Symbol List: {processed_symbols}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
